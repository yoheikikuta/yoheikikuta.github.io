---
title: 自分が感銘を受けた ML/AI トピック 3 選
description: これまでの ML/AI の経験で自分が最も感銘を受けたトピックを 3 つ紹介するブログ記事。
pubDate: 2024-09-29
tags: ['Machine Learning']
---

### TL;DR
- 2014 年から 10 年くらい ML/AI 領域に関与してきたので、自分が感銘を受けたトピックを振り返って 3 つ挙げてみる
- 自分としては BERT と Alphafold と OpenAI o1 で、次点は Adversarial Attacks かな
- こういうのは他の人のも聞いてみると面白いので聞いてみたいね
---

2014 年 4 月から社会人になって、10 年程度 ML/AI 領域の仕事をしてきた。
振り返ってみると、時代的にも盛り上がったこともあり、10 年の間に色々なトピックがあった。

OpenAI o1 が出てきてこれは圧倒的に自分が感銘を受けたものトップ 3 に入るなと思ったけど、そう思ってからトップ 3 って他はなんだろうなとなったので、この 10 年程度を振り返った時に自分が特に感銘を受けたトピック 3 選を書いてみる。


### BERT (2018年)
BERT　[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805) が出てきて、これで自然言語処理のタスクをかなり解くことができそうとワクワクしたのを覚えている。
多様な自然言語処理のタスクを一つのアーキテクチャで高性能に解くというという流れが活発になっていた時で、この後に例えば T5 とかが出てテキストでタスクを指定して解けるようになってきて、GPT-3,4 で決定版になった、という大きな潮流の中で重要な役割を果たしたものだと思う。

当時は自然言語処理系のモデルでいきなり多言語対応のものが出るということは少なく、BERT 単体というより、SentencePiece なども出てきてたことで、様々な言語で様々なタスクが一気に解けるようになってきたというのが exciting なところだった。

これは自分も参加するしかないと思って作ったのが [https://github.com/yoheikikuta/bert-japanese](https://github.com/yoheikikuta/bert-japanese) だった。
各時代で様々な面白さがあるけど、この時は個人で最先端のモデルを作れる時代だったし、今後より発展していきそうな領域で自分が主体的に関われていたのが自分にとって印象深いものとなった理由だろう。


### AlphaFold (2021年)
[https://www.nature.com/articles/s41586-021-03819-2](https://www.nature.com/articles/s41586-021-03819-2) なので 2021 年としてる（元々前身の AlphFold があってこれは AlphaFold2 だがナンバリングがなくなってこれが AlphaFold と呼ばれるようになっている）。

ドメイン知識と機械学習の最高レベルの融合で圧倒的な成果を出したという意味でインパクトが大きかった。
これは凄そうだと思って論文を読んだ [https://github.com/yoheikikuta/paper-reading/issues/60](https://github.com/yoheikikuta/paper-reading/issues/60) が、ドメイン知識の観点で足りないところが多かったので、Twitter で雑に podcast で話したいと募集したら専門家の [@Ag_smith](https://x.com/Ag_smith) 氏が参加してくれて podcast を収録した [https://podcasters.spotify.com/pod/show/yoheikikuta/episodes/16AlphaFold-e15ubme](https://podcasters.spotify.com/pod/show/yoheikikuta/episodes/16AlphaFold-e15ubme)。

自分は全然ドメイン知識を持ってなくて理解が及ばないところも多々あったけど、それでも論文を読んで実に色々な要素を作り込んでいたのは圧倒されたし、こういうのが素晴らしい研究ってものだよなと感動した。
ドメイン知識が薄い自分ですらトップ 3 に入るほどの感銘を受けたので、専門領域に近い人たちにとってはきっとものすごいインパクトであっただろう。

化学賞なのか生理学・医学賞なのかはよく分からないけど、そのうちノーベル賞も獲るのだろうと思う。


### OpenAI o1-preview,mini (2024年)
GPT-3,4 とかももちろん感動したんだけど、自分としてはそれ以上に o1 に感動した。

生成モデルの特性上、論理的な推論はかなり難易度が高いと思っていたけど、o1 が出てきて触ってみて想像を遥かに超えるレベルだったのはとてもテンションが上がった。

自分は数理的な話が好きだけど、そういう分野でこれだけ汎用的に機能するものがこれだけ早く出てくるとは思ってなかった。
現状はまだ自分が理解を深めるための良きパートナーになるというレベルだけど、近い将来に自分の好きな領域において圧倒的に自分より賢くなるのは確実なので、なんというか、感慨深い。

AGI/ASI やら先の未来の進化が煽り気味に語られることも多いけど、論理的な推論という能力においてもほとんどの人類より既に高いレベルに到達しているという事実は、改めて考えてみても凄いことだなと思う。

いや〜しかし良い時代に生まれましたね。


### その他のものたち
現象として興味深くて色々と追ったりしたのは Adversarial Attacks 関連で、これは色々調べたり技術同人誌を書いたりした [https://github.com/yoheikikuta/a-primer-on-adversarial-examples](https://github.com/yoheikikuta/a-primer-on-adversarial-examples) ので印象に残ってるけど、トップ 3 のインパクトが強すぎたので次点という感じ。

レコメンドで XGBoost を production で使うとか、画像認識で新しいモデルを fine-tuning して production で使うとか、そういうことも楽しかったけど、思い返してみると最上位に来るほどではなかった。
キャリア的には比較的初期の話なので、単にその時の感動を忘れているだけの可能性もあるが。

10 年振り返ってみてめちゃくちゃ色々あったけど、この先の 10 年でもめちゃくちゃ色々起こりそうなので楽しみだね。
